{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee90eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f5676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14fb94221944e13a48529bac1754979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34cc88c1e5d4c918cb516b6ab88cfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cea8c5ef93b4605abb596aa0f065f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfb426503f246fcac891316ad9752ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcd720de4ae4a5d803b8cf530786d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Notebook Block 1: Setup & configuration ---\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# >> Change if needed <<\n",
    "INPUT_JSON_PATH = Path(\"virtualHome_raw.json\")   # your input file\n",
    "OUTPUT_JSON_PATH = Path(\"virtualHome_gemma-3-4b-pt.json\")\n",
    "\n",
    "# Make the tokenizer easy to change\n",
    "TOKENIZER_NAME = \"google/gemma-3-4b-pt\"  # e.g., \"google/gemma-3-4b\" or any other HF tokenizer id\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aad409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook Block 2: Utilities: matching & span mapping ---\n",
    "\n",
    "def find_plan_start(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the character index immediately after the first occurrence of 'Plan:' line.\n",
    "    If not found, returns 0 (i.e., analyze whole text).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\bPlan:\\s*\\n\", text)\n",
    "    return m.end() if m else 0\n",
    "\n",
    "def sequential_find(text: str, needles: List[str], start_at: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Sequentially find each needle in order inside text starting at start_at.\n",
    "    Returns list of (char_start, char_end) for each needle.\n",
    "    If any is missing, returns an empty list.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    pos = start_at\n",
    "    for needle in needles:\n",
    "        i = text.find(needle, pos)\n",
    "        if i < 0:\n",
    "            return []\n",
    "        spans.append((i, i + len(needle)))\n",
    "        pos = i + len(needle)\n",
    "    return spans\n",
    "\n",
    "def char_to_token_span(text: str, match_span: Tuple[int, int], offsets: List[Tuple[int, int]]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Map a character span [start_char, end_char) to token indices [start_tok, end_tok),\n",
    "    such that tokens[start_tok:end_tok] cover exactly that char span.\n",
    "    We pick:\n",
    "      - start_tok: first token whose offset_start <= match_start < offset_end\n",
    "      - end_tok: first token index AFTER the last token overlapping the span\n",
    "    Returns (start_tok, end_tok). If not found, raises ValueError.\n",
    "    \"\"\"\n",
    "    m_start, m_end = match_span\n",
    "    start_tok = None\n",
    "    end_tok_exclusive = None\n",
    "\n",
    "    # Find start token\n",
    "    for ti, (cs, ce) in enumerate(offsets):\n",
    "        if cs <= m_start < ce:\n",
    "            start_tok = ti\n",
    "            break\n",
    "    if start_tok is None:\n",
    "        # Edge case: match may start exactly at a boundary where a token offset has ce == cs\n",
    "        for ti, (cs, ce) in enumerate(offsets):\n",
    "            if cs == m_start and cs == ce:\n",
    "                start_tok = ti\n",
    "                break\n",
    "    if start_tok is None:\n",
    "        raise ValueError(\"Could not map start char to a token index.\")\n",
    "\n",
    "    # Find end token (exclusive): the first token whose offset_start >= m_end\n",
    "    # If none, it's the length of offsets (span ends at last token)\n",
    "    for ti, (cs, ce) in enumerate(offsets[start_tok:], start=start_tok):\n",
    "        if cs >= m_end:\n",
    "            end_tok_exclusive = ti\n",
    "            break\n",
    "    if end_tok_exclusive is None:\n",
    "        end_tok_exclusive = len(offsets)\n",
    "\n",
    "    return start_tok, end_tok_exclusive\n",
    "\n",
    "def encode_with_offsets(text: str):\n",
    "    \"\"\"\n",
    "    Tokenize and return (input_ids, offsets) where offsets is a list of (start_char, end_char).\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    offsets = enc[\"offset_mapping\"]\n",
    "    return enc[\"input_ids\"], offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d4def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook Block 3: Per-sample processing ---\n",
    "\n",
    "def process_sample(sample: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Recomputes start_token_idx and end_token_idx for this sample using the configured tokenizer.\n",
    "    Searches only the portion of text after 'Plan:\\n'.\n",
    "    Returns a NEW sample dict (does not mutate the input).\n",
    "    \"\"\"\n",
    "    text = sample.get(\"text\", \"\")\n",
    "    targets: List[str] = sample.get(\"tar_eq\", [])  # assuming field name is tar_eq\n",
    "    if not isinstance(targets, list):\n",
    "        targets = []\n",
    "\n",
    "    # Locate the region after 'Plan:\\n'\n",
    "    plan_start_char = find_plan_start(text)\n",
    "\n",
    "    # Sequentially match targets\n",
    "    match_spans = sequential_find(text, targets, plan_start_char)\n",
    "\n",
    "    # If any target not found, leave indices empty but keep text & tar_eq\n",
    "    if not match_spans:\n",
    "        return {\n",
    "            **sample,\n",
    "            \"start_token_idx\": [],\n",
    "            \"end_token_idx\": []\n",
    "        }\n",
    "\n",
    "    # Tokenize entire text (we'll map with global char offsets)\n",
    "    _, offsets = encode_with_offsets(text)\n",
    "\n",
    "    start_tok_list: List[int] = []\n",
    "    end_tok_list: List[int] = []\n",
    "\n",
    "    for span in match_spans:\n",
    "        try:\n",
    "            st, et = char_to_token_span(text, span, offsets)\n",
    "            # The userâ€™s example uses exclusive end? Their example had element-wise pairs;\n",
    "            # We'll keep indices as token-level [start, end) (exclusive end), which is standard.\n",
    "            start_tok_list.append(st)\n",
    "            end_tok_list.append(et)\n",
    "        except ValueError:\n",
    "            # If mapping fails, bail out for this sample with empty indices\n",
    "            return {\n",
    "                **sample,\n",
    "                \"start_token_idx\": [],\n",
    "                \"end_token_idx\": []\n",
    "            }\n",
    "\n",
    "    # Return updated sample with *new* indices; text and tar_eq unchanged\n",
    "    return {\n",
    "        **sample,\n",
    "        \"start_token_idx\": start_tok_list,\n",
    "        \"end_token_idx\": end_tok_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b822d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 247 samples to: virtualHome_gemma-3-4b-pt.json\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Block 4: Batch processing & save ---\n",
    "\n",
    "def load_dataset(path: Path) -> List[dict]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Input JSON must be a list of samples.\")\n",
    "    return data\n",
    "\n",
    "def save_dataset(path: Path, data: List[dict]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def recompute_indices(input_path: Path, output_path: Path) -> None:\n",
    "    samples = load_dataset(input_path)\n",
    "    out = []\n",
    "    missing = 0\n",
    "    for s in samples:\n",
    "        new_s = process_sample(s)\n",
    "        if not new_s.get(\"start_token_idx\") or not new_s.get(\"end_token_idx\"):\n",
    "            missing += 1\n",
    "        out.append(new_s)\n",
    "\n",
    "    save_dataset(output_path, out)\n",
    "    print(f\"Saved {len(out)} samples to: {output_path}\")\n",
    "    if missing:\n",
    "        print(f\"Note: {missing} sample(s) had some targets not found or mapping issues; indices left empty.\")\n",
    "\n",
    "# Run\n",
    "recompute_indices(INPUT_JSON_PATH, OUTPUT_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dce110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar_eq: ['[WALK]', '<home_office>', '[WALK]', '<chair>', '[FIND]', '<chair>', '[SIT]', '<chair>', '[FIND]', '<phone>', '[GRAB]', '<phone>', '[END]']\n",
      "start_token_idx: [117, 121, 127, 487, 491, 494, 498, 502, 506, 697, 701, 705, 709]\n",
      "end_token_idx: [121, 126, 131, 490, 494, 497, 502, 505, 509, 700, 705, 708, 712]\n",
      "      [WALK] -> tokens[117:121] == '[WALK]'\n",
      "<home_office> -> tokens[121:126] == ' <home_office>'\n",
      "      [WALK] -> tokens[127:131] == '[WALK]'\n",
      "     <chair> -> tokens[487:490] == ' <chair>'\n",
      "      [FIND] -> tokens[491:494] == '[FIND]'\n",
      "     <chair> -> tokens[494:497] == ' <chair>'\n",
      "       [SIT] -> tokens[498:502] == '[SIT]'\n",
      "     <chair> -> tokens[502:505] == ' <chair>'\n",
      "      [FIND] -> tokens[506:509] == '[FIND]'\n",
      "     <phone> -> tokens[697:700] == ' <phone>'\n",
      "      [GRAB] -> tokens[701:705] == '[GRAB]'\n",
      "     <phone> -> tokens[705:708] == ' <phone>'\n",
      "       [END] -> tokens[709:712] == '[END]'\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Block 5: Sanity check (optional) ---\n",
    "\n",
    "data = json.load(open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\"))\n",
    "first = process_sample(data[0])\n",
    "print(\"tar_eq:\", first[\"tar_eq\"])\n",
    "print(\"start_token_idx:\", first[\"start_token_idx\"])\n",
    "print(\"end_token_idx:\", first[\"end_token_idx\"])\n",
    "\n",
    "# Show token strings for each span for visual verification\n",
    "text = first[\"text\"]\n",
    "_, offsets = encode_with_offsets(text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def tokens_for_span(st, et):\n",
    "    return \"\".join(text[s:e] for (s, e) in offsets[st:et])\n",
    "\n",
    "for t, st, et in zip(first[\"tar_eq\"], first[\"start_token_idx\"], first[\"end_token_idx\"]):\n",
    "    print(f\"{t:>12} -> tokens[{st}:{et}] == '{tokens_for_span(st, et)}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmt_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
