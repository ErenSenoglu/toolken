{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee90eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5f5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 1: Setup & configuration ---\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths\n",
    "INPUT_JSON_PATH = Path(\"virtualHome_raw.json\")\n",
    "OUTPUT_JSON_PATH = Path(\"virtualHome_gemma-3-4b-pt.json\")\n",
    "\n",
    "# Tokenizer (easy to change)\n",
    "TOKENIZER_NAME = \"google/gemma-3-4b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5aad409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 2: Tokenization & mapping utilities ---\n",
    "\n",
    "def encode_with_offsets(text: str):\n",
    "    enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    return enc[\"input_ids\"], enc[\"offset_mapping\"]\n",
    "\n",
    "def char_to_token_span(match_span: Tuple[int, int], offsets: List[Tuple[int, int]]) -> Tuple[int, int]:\n",
    "    m_start, m_end = match_span\n",
    "    start_tok = None\n",
    "    for ti, (cs, ce) in enumerate(offsets):\n",
    "        if cs <= m_start < ce:\n",
    "            start_tok = ti\n",
    "            break\n",
    "    if start_tok is None:\n",
    "        # Very rare boundary case; fallback to first token starting at m_start\n",
    "        for ti, (cs, ce) in enumerate(offsets):\n",
    "            if cs == m_start:\n",
    "                start_tok = ti\n",
    "                break\n",
    "    if start_tok is None:\n",
    "        raise ValueError(\"Start char not mapped to any token.\")\n",
    "\n",
    "    end_tok_excl = None\n",
    "    for ti in range(start_tok, len(offsets)):\n",
    "        cs, ce = offsets[ti]\n",
    "        if cs >= m_end:\n",
    "            end_tok_excl = ti\n",
    "            break\n",
    "    if end_tok_excl is None:\n",
    "        end_tok_excl = len(offsets)\n",
    "    return start_tok, end_tok_excl\n",
    "\n",
    "def find_plan_start(text: str) -> int:\n",
    "    m = re.search(r\"\\bPlan:\\s*\\n?\", text)\n",
    "    return m.end() if m else 0\n",
    "\n",
    "def sequential_find(text: str, needles: List[str], start_at: int) -> List[Tuple[int, int]]:\n",
    "    spans, pos = [], start_at\n",
    "    for needle in needles:\n",
    "        i = text.find(needle, pos)\n",
    "        if i < 0:\n",
    "            return []\n",
    "        spans.append((i, i + len(needle)))\n",
    "        pos = i + len(needle)\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d4def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 3: Task parsing & exact Plan matching ---\n",
    "\n",
    "TASK_HDR_RE = re.compile(r\"(Task\\s*\\d+:)\", flags=re.IGNORECASE)\n",
    "\n",
    "def split_tasks(text: str) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Returns list of (task_text, start_char, end_char) within the original text.\n",
    "    \"\"\"\n",
    "    matches = list(TASK_HDR_RE.finditer(text))\n",
    "    if not matches:\n",
    "        return []\n",
    "    tasks = []\n",
    "    for i, m in enumerate(matches):\n",
    "        start = m.start()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        tasks.append((text[start:end], start, end))\n",
    "    return tasks\n",
    "\n",
    "def extract_plan_block(task_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the substring from 'Plan:' to the end of the task_text.\n",
    "    (Empirically, Plan is last; if not, this still suffices.)\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\bPlan:\\s*\\n?\", task_text)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return task_text[m.end():]\n",
    "\n",
    "def parse_plan_tags(plan_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts tags in order: tokens like [ACTION] or <object>.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"(\\[[^\\]]+\\]|<[^>]+>)\", plan_text)\n",
    "\n",
    "def choose_exact_task(text: str, tar_eq: List[str]) -> Optional[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Returns (kept_task_text, plan_start_offset_within_kept_task) if a task whose Plan\n",
    "    tags equal tar_eq exactly is found; else None.\n",
    "    \"\"\"\n",
    "    for task_text, _, _ in split_tasks(text):\n",
    "        plan = extract_plan_block(task_text)\n",
    "        tags = parse_plan_tags(plan)\n",
    "        if tags == tar_eq:\n",
    "            # Plan start offset within the kept task text\n",
    "            plan_start = find_plan_start(task_text)\n",
    "            return task_text, plan_start\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b822d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 4: Per-sample processing ---\n",
    "\n",
    "def process_sample(sample: dict) -> dict:\n",
    "    text: str = sample.get(\"text\", \"\")\n",
    "    tar_eq: List[str] = list(sample.get(\"tar_eq\", []))\n",
    "    if not isinstance(tar_eq, list):\n",
    "        tar_eq = []\n",
    "\n",
    "    # Try to pick the one and only Task whose Plan exactly equals tar_eq\n",
    "    choice = choose_exact_task(text, tar_eq)\n",
    "\n",
    "    if choice is None:\n",
    "        # No strict match found\n",
    "        kept_text = text\n",
    "        strict = [False] * len(tar_eq)\n",
    "        start_token_idx, end_token_idx = [], []\n",
    "        # We still return with text unchanged and empty indices\n",
    "        return {\n",
    "            **sample,\n",
    "            \"text\": kept_text,\n",
    "            \"start_token_idx\": start_token_idx,\n",
    "            \"end_token_idx\": end_token_idx,\n",
    "            \"strict_match\": strict,\n",
    "        }\n",
    "\n",
    "    kept_task_text, plan_start = choice\n",
    "    strict = [True] * len(tar_eq)\n",
    "\n",
    "    # Tokenize kept task text and map tar_eq spans starting AFTER \"Plan:\"\n",
    "    _, offsets = encode_with_offsets(kept_task_text)\n",
    "\n",
    "    # Compute char spans of tar_eq within the kept task text\n",
    "    spans = sequential_find(kept_task_text, tar_eq, plan_start)\n",
    "    if not spans:\n",
    "        # Shouldn't happen if tags were parsed from this task, but guard anyway\n",
    "        return {\n",
    "            **sample,\n",
    "            \"text\": kept_task_text,\n",
    "            \"start_token_idx\": [],\n",
    "            \"end_token_idx\": [],\n",
    "            \"strict_match\": [False] * len(tar_eq),\n",
    "        }\n",
    "\n",
    "    start_token_idx, end_token_idx = [], []\n",
    "    try:\n",
    "        for span in spans:\n",
    "            st, et = char_to_token_span(span, offsets)\n",
    "            start_token_idx.append(st)\n",
    "            end_token_idx.append(et)\n",
    "    except ValueError:\n",
    "        # Fallback on mapping issues\n",
    "        start_token_idx, end_token_idx = [], []\n",
    "        strict = [False] * len(tar_eq)\n",
    "\n",
    "    return {\n",
    "        **sample,\n",
    "        \"text\": kept_task_text,\n",
    "        \"start_token_idx\": start_token_idx,\n",
    "        \"end_token_idx\": end_token_idx,\n",
    "        \"strict_match\": strict,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dce110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 247 samples to: virtualHome_gemma-3-4b-pt.json\n"
     ]
    }
   ],
   "source": [
    "# --- Block 5: Batch processing & save ---\n",
    "\n",
    "def load_dataset(path: Path) -> List[dict]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Input JSON must be a list of samples.\")\n",
    "    return data\n",
    "\n",
    "def save_dataset(path: Path, data: List[dict]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def recompute_indices(input_path: Path, output_path: Path) -> None:\n",
    "    samples = load_dataset(input_path)\n",
    "    out = []\n",
    "    no_match = 0\n",
    "    for s in samples:\n",
    "        new_s = process_sample(s)\n",
    "        if not all(new_s.get(\"strict_match\", [])):\n",
    "            no_match += 1\n",
    "        out.append(new_s)\n",
    "\n",
    "    save_dataset(output_path, out)\n",
    "    print(f\"Saved {len(out)} samples to: {output_path}\")\n",
    "    if no_match:\n",
    "        print(f\"Note: {no_match} sample(s) had no exact Task/Plan match or mapping issues (strict_match not all True).\")\n",
    "\n",
    "# Run\n",
    "recompute_indices(INPUT_JSON_PATH, OUTPUT_JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45b1761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict_match: [True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "start_token_idx: [129, 133, 139, 143, 147, 150, 154, 158, 162, 165, 169, 173, 177]\n",
      "end_token_idx: [133, 138, 143, 146, 150, 153, 158, 161, 165, 168, 173, 176, 180]\n",
      "\n",
      "Kept text preview:\n",
      " Task 4:\n",
      "I am in ['dining_room']. The objects I can manipulate are ['check', 'chair', 'mouse', 'cupboard', 'bedroom', 'home_office', 'food_food', 'freezer', 'keyboard', 'faucet', 'mail', 'television', 'novel', 'light', 'couch', 'desk', 'phone', 'computer', 'table', 'dining_room', 'sink', 'bathroom', 'bed'].\n",
      "Goal:\n",
      "Pick up phone\n",
      "Hint:\n",
      "When the phone rings, I pick up the call and give response to the \n"
     ]
    }
   ],
   "source": [
    "# --- Block 6: Sanity check (optional) ---\n",
    "\n",
    "data = load_dataset(INPUT_JSON_PATH)\n",
    "if data:\n",
    "    first_out = process_sample(data[0])\n",
    "    print(\"strict_match:\", first_out.get(\"strict_match\"))\n",
    "    print(\"start_token_idx:\", first_out.get(\"start_token_idx\"))\n",
    "    print(\"end_token_idx:\", first_out.get(\"end_token_idx\"))\n",
    "    print(\"\\nKept text preview:\\n\", first_out.get(\"text\")[:400])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmt_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
