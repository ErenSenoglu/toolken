{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dacab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9bf1a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"asdiv\"\n",
    "llm = \"gemma-3-4b-pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0ff99c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5afcfebe83942ac85d4b8c130f6cca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4e42057af841289abbc79d6e1745ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1240\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 310\n",
      "    })\n",
      "})\n",
      "{'question': 'Joanne is campaigning for class president and plans to distribute some campaign materials: 20 flyers and 16 buttons. She wants each classroom to receive an identical set of campaign materials, without having any materials left over. What is the greatest number of classrooms Joanne can distribute materials to?', 'answer': 'The final answer to the problem is GCD(20, 16) = <<gcd(20, 16)=4>>4'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import re\n",
    "\n",
    "# 1) Load ASDiv\n",
    "raw = load_dataset(\"EleutherAI/asdiv\", trust_remote_code=True)\n",
    "\n",
    "# 2) Helpers\n",
    "num_re = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?\")\n",
    "allowed_re = re.compile(r\"^[0-9+\\-*/=() ]+$\")  # allowed arithmetic chars\n",
    "gcd_re = re.compile(r\"^GCD\\((\\d+),\\s*(\\d+)\\)\\s*=\\s*(\\d+)$\", re.IGNORECASE)\n",
    "\n",
    "def build_answer_from_formula(formula: str, fallback_answer_text: str) -> str:\n",
    "    if formula is None:\n",
    "        return None\n",
    "    f = formula.strip()\n",
    "\n",
    "    # ✅ Case 1: Handle GCD explicitly\n",
    "    m = gcd_re.match(f)\n",
    "    if m:\n",
    "        n, m_, k = m.groups()\n",
    "        return f\"The final answer to the problem is GCD({n}, {m_}) = <<gcd({n}, {m_})={k}>>{k}\"\n",
    "\n",
    "    # ✅ Case 2: Pure arithmetic expression\n",
    "    f_no_space = f.replace(\" \", \"\")\n",
    "    if not f_no_space:\n",
    "        return None\n",
    "\n",
    "    if not allowed_re.match(f_no_space):\n",
    "        return None  # discard if invalid\n",
    "\n",
    "    if \"=\" in f_no_space:\n",
    "        lhs, rhs = f_no_space.split(\"=\", 1)\n",
    "        return f\"The final answer to the problem is {lhs} = <<{lhs}={rhs}>>{rhs}\"\n",
    "    else:\n",
    "        # fallback: try to grab numeric answer from textual answer\n",
    "        nums = num_re.findall(fallback_answer_text or \"\")\n",
    "        if not nums:\n",
    "            return None\n",
    "        rhs = nums[-1]\n",
    "        lhs = f_no_space\n",
    "        return f\"The final answer to the problem is {lhs} = <<{lhs}={rhs}>>{rhs}\"\n",
    "\n",
    "def concat_question(body: str, question: str) -> str:\n",
    "    body = (body or \"\").strip()\n",
    "    q = (question or \"\").strip()\n",
    "    if body and not body.endswith((\".\", \"?\", \"!\")):\n",
    "        body = body + \".\"\n",
    "    return (body + \" \" + q).strip()\n",
    "\n",
    "# 3) Transform function\n",
    "def transform(example):\n",
    "    question = concat_question(example.get(\"body\", \"\"), example.get(\"question\", \"\"))\n",
    "    answer = build_answer_from_formula(example.get(\"formula\", \"\"), example.get(\"answer\", \"\"))\n",
    "    if answer is None:\n",
    "        return None  # discard only if not arithmetic and not GCD\n",
    "    return {\"question\": question, \"answer\": answer}\n",
    "\n",
    "# 4) Apply transform\n",
    "transformed = raw[\"validation\"].map(\n",
    "    transform,\n",
    "    remove_columns=raw[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# Drop invalid rows (transform returned None)\n",
    "transformed = transformed.filter(lambda x: x is not None and x[\"answer\"] is not None)\n",
    "\n",
    "# 5) Split 80/20\n",
    "split = transformed.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 6) Final dataset\n",
    "ds = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"test\": split[\"test\"],\n",
    "})\n",
    "\n",
    "# 7) Inspect\n",
    "print(ds)\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dfc466f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"google/{llm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cbea7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tool_call_pattern = re.compile(r'(<<[^>]+>>)([^<]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f7a1bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(49, 67), match='<<gcd(20, 16)=4>>4'>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tool_call_pattern.finditer(ds['train'][0]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2e88ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_call(expression: str):\n",
    "    \"\"\"\n",
    "    Parses a mathematical expression and converts it into the ToolkenGPT format.\n",
    "    Examples: \n",
    "    \"48/2=24\" -> \"<divide>(48, 2)=24\"\n",
    "    \"+30+46+38+11+18=143\" -> \"<add>(30, 46, 38, 11, 18)=143\"\n",
    "    \"\"\"\n",
    "    # Simple mapping from operator to function name\n",
    "    op_to_func = {\n",
    "        '+': 'add',\n",
    "        '-': 'subtract',\n",
    "        '*': 'multiply',\n",
    "        '/': 'divide',\n",
    "        'GCD': 'gcd'\n",
    "    }\n",
    "\n",
    "    # Find the operator and split on equals sign\n",
    "    if '=' in expression:\n",
    "        expr_part, result = expression.split('=', 1)\n",
    "        result = result.strip()\n",
    "        \n",
    "        # Handle multiple additions/subtractions\n",
    "        for op, func_name in op_to_func.items():\n",
    "            if op in expr_part:\n",
    "                # Skip if it's just a leading sign\n",
    "                if expr_part.strip() == op:\n",
    "                    continue\n",
    "                    \n",
    "                # Split on operator, filter out empty strings\n",
    "                parts = [p.strip() for p in expr_part.split(op) if p.strip()]\n",
    "                \n",
    "                # If we found valid parts, convert to function call format\n",
    "                if parts:\n",
    "                    return f\"<{func_name}>({', '.join(parts)})={result}\"\n",
    "    \n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "55a6ba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<add>(30, 46, 18)=100'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_tool_call(\"30+46+18=100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "111d3791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The final answer to the problem is 344-136 = <<344-136=208>>208'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][108]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b9bafc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index by searching through processed samples\n",
    "'''found_idx = next((i for i, sample in enumerate(processed_samples) \n",
    "                  if sample['text'] == original[108]['text']), None)\n",
    "print(found_idx)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec2436c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The final answer to the problem is (43-31)/6 = <<(43-31)/6=2>>2'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][140][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c0523de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original[108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f0586b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1240/1240 [00:00<00:00, 2423.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch ratio: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "verbose = 1\n",
    "mismatches = 0\n",
    "\n",
    "\n",
    "processed_samples = []\n",
    "\n",
    "\n",
    "for sample in tqdm(ds['train']):\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    target_equations = []\n",
    "    target_numbers = []\n",
    "        \n",
    "    removed_chars_offset = 0\n",
    "\n",
    "    full_text = sample['question'] + \" Let's think step by step. \" + sample['answer']\n",
    "\n",
    "    matches = list(tool_call_pattern.finditer(full_text))\n",
    "\n",
    "    # Create the final clean text by removing only the <<...>> syntax.\n",
    "    clean_text = re.sub(r'<<[^>]+>>', '', full_text)\n",
    "\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    target_equations = []\n",
    "    target_numbers = []\n",
    "\n",
    "    removed_chars_offset = 0\n",
    "\n",
    "    for match in matches:\n",
    "        expression_part = match.group(1) # The <<...>> part\n",
    "    \n",
    "        expression = expression_part[2:-2] # The content inside\n",
    "        expression = re.sub(r'(?<=[\\s=+\\-*/])\\.(\\d+)', r'0.\\1', expression)  # Add 0 before decimal points\n",
    "\n",
    "        following_text = match.group(2) # The text after <<...>>\n",
    "        \n",
    "        # The pattern should handle negative numbers as well.\n",
    "        number_pattern = re.compile(r'-?[\\d,]*\\.?\\d+')\n",
    "        num_match = number_pattern.search(following_text)\n",
    "        num = num_match.group(0) if num_match else None\n",
    "        expected_num_str = expression.split('=')[-1].strip()\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assert num == expected_num_str\n",
    "        except AssertionError:\n",
    "            mismatches += 1\n",
    "            print(\"-\"*25)\n",
    "            print(num_match)\n",
    "            print(f\"Full text: {sample['answer']}\")\n",
    "            print(\"/\"*10)\n",
    "            print(f\"Found match: {match.group(0)}\")\n",
    "            print(f\"Expression part: {expression_part}\")\n",
    "            print(f\"Following text: {following_text}\")\n",
    "            print(f\"Number found: {num}\")\n",
    "            print(f\"Expected number from expression: {expected_num_str}\")\n",
    "        \"\"\"\n",
    "        # 1. Calculate the character position of where the number *starts* in the clean_text.\n",
    "        try:\n",
    "            char_pos_start = (match.start() - removed_chars_offset) + num_match.start()\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Tokenize the clean text *before* the number's position to find the start_idx.\n",
    "        text_before_result = clean_text[:char_pos_start]\n",
    "        tokens_before = tokenizer.encode(text_before_result)\n",
    "        start_idx = len(tokens_before)\n",
    "\n",
    "        # 3. Calculate the character position of where the number *ends* in the clean_text.\n",
    "        char_pos_end = char_pos_start + len(num)\n",
    "        \n",
    "        # 4. Tokenize the clean text up to the *end* of the number. The length of this\n",
    "        #    token sequence is our end_idx.\n",
    "        text_up_to_end_of_result = clean_text[:char_pos_end]\n",
    "        tokens_up_to_end = tokenizer.encode(text_up_to_end_of_result)\n",
    "        end_idx = len(tokens_up_to_end)\n",
    "\n",
    "        # 5. Store the start and end indices for the current match.\n",
    "        start_indices.append(start_idx)\n",
    "        end_indices.append(end_idx)\n",
    "        target_equations.append(parse_tool_call(expression))\n",
    "\n",
    "        target_numbers.append(num)\n",
    "\n",
    "        # Update the offset for the next iteration by adding the length of the\n",
    "        # <<...>> syntax string we just processed.\n",
    "        removed_chars_offset += len(expression_part)\n",
    "\n",
    "        if num != expected_num_str and False:\n",
    "            print(f\"Text: {clean_text}\")\n",
    "            print(f\"Before: {text_before_result}\")\n",
    "            print(f\"After: {text_up_to_end_of_result}\")\n",
    "            print(f\"Target equations: {target_equations}\")\n",
    "    \n",
    "    processed_samples.append({\n",
    "            \"text\": clean_text,\n",
    "            \"start_token_idx\": start_indices,\n",
    "            \"end_token_idx\": end_indices,\n",
    "            \"tar_eq\": [eq.replace(\"gcd\", \"<gcd>\") for eq in target_equations],\n",
    "            \"tar_number\": target_numbers,\n",
    "        })\n",
    "\n",
    "print(f\"Mismatch ratio: {mismatches/len(ds['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b3b5771d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<234/6=39>>'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f5a8c28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'234/6=39'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31f41502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "following_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "24f66b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(mismatches_after_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dff6cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict — total: 1240, matches (with correction): 1240, mismatches: 0\n"
     ]
    }
   ],
   "source": [
    "# Strict validation: update samples in-place with corrected indices and match status.\n",
    "strict_total = 0\n",
    "strict_ok = 0\n",
    "mismatches_after_correction = []\n",
    "\n",
    "for i, samp in enumerate(processed_samples):\n",
    "    # Initialize a new list to store the match status for each tool call in the sample\n",
    "    samp[\"strict_match\"] = []\n",
    "    \n",
    "    # keep tokenizer behavior consistent with above preprocessing (same encode)\n",
    "    token_ids = tokenizer.encode(samp[\"text\"])\n",
    "    \n",
    "    # Enumerate to get the index 'j' for updating the lists\n",
    "    for j, (s, t, num) in enumerate(zip(samp[\"start_token_idx\"], samp[\"end_token_idx\"], samp.get(\"tar_number\", []))):\n",
    "        strict_total += 1\n",
    "        span_text = tokenizer.decode(token_ids[s:t])\n",
    "        span_text_m1 = tokenizer.decode(token_ids[s-1:t]).strip()\n",
    "        span_text_p1 = tokenizer.decode(token_ids[s+1:t]).strip()\n",
    "\n",
    "        is_match = False\n",
    "\n",
    "        if (span_text == num):\n",
    "            strict_ok += 1\n",
    "            is_match = True\n",
    "        # Check for a match with a left-shifted start index\n",
    "        elif (span_text_m1 == num):\n",
    "            strict_ok += 1\n",
    "            is_match = True\n",
    "            # Correct the start index in the sample\n",
    "            samp[\"start_token_idx\"][j] -= 1\n",
    "        # Check for a match with a right-shifted start index\n",
    "        elif (span_text_p1 == num):\n",
    "            strict_ok += 1\n",
    "            is_match = True\n",
    "            # Correct the start index in the sample\n",
    "            samp[\"start_token_idx\"][j] += 1\n",
    "        \n",
    "        # Append the match status (True or False) for the current tool call\n",
    "        samp[\"strict_match\"].append(is_match)\n",
    "\n",
    "        if not is_match:\n",
    "            mismatches_after_correction.append({\n",
    "                \"sample_idx\": i,\n",
    "                \"tool_call_idx\": j,\n",
    "                \"s\": s,\n",
    "                \"t\": t,\n",
    "                \"expected\": num,\n",
    "                \"span_text\": span_text,\n",
    "                \"ctx_left\": tokenizer.decode(token_ids[max(0, s-5):s]),\n",
    "                \"ctx_right\": tokenizer.decode(token_ids[t:min(len(token_ids), t+5)]),\n",
    "            })\n",
    "\n",
    "print(f\"Strict — total: {strict_total}, matches (with correction): {strict_ok}, mismatches: {len(mismatches_after_correction)}\")\n",
    "\n",
    "# Show a few remaining mismatches for inspection\n",
    "for r in mismatches_after_correction[:10]:\n",
    "    print(\"-\"*50)\n",
    "    print({k: r[k] for k in [\"sample_idx\", \"tool_call_idx\", \"s\", \"t\", \"expected\", \"span_text\"]})\n",
    "    print(\"L:\", r[\"ctx_left\"])\n",
    "    print(\"R:\", r[\"ctx_right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a8c0c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Joanne is campaigning for class president and plans to distribute some campaign materials: 20 flyers and 16 buttons. She wants each classroom to receive an identical set of campaign materials, without having any materials left over. What is the greatest number of classrooms Joanne can distribute materials to? Let's think step by step. The final answer to the problem is GCD(20, 16) = 4\",\n",
       " 'start_token_idx': [85],\n",
       " 'end_token_idx': [86],\n",
       " 'tar_eq': ['<gcd>(20, 16)=4'],\n",
       " 'tar_number': ['4'],\n",
       " 'strict_match': [True]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "05c956b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data saved to outputs/asdiv_gemma-3-4b-pt.json\n",
      "✅ Function dictionary saved to outputs/func_dict.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# --- Save the processed data ---\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(\"outputs\", f\"{dataset}_{llm}.json\")\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(processed_samples, f, indent=4)\n",
    "print(f\"✅ Processed data saved to {output_path}\")\n",
    "\n",
    "# --- Create and save the final function dictionary ---\n",
    "func_dict = {\n",
    "    \"<add>\": 0,\n",
    "    \"<subtract>\": 1,\n",
    "    \"<multiply>\": 2,\n",
    "    \"<divide>\": 3,\n",
    "    \"<gcd>\": 4\n",
    "}\n",
    "func_dict_path = os.path.join(\"outputs\", \"func_dict.json\")\n",
    "with open(func_dict_path, 'w') as f:\n",
    "    json.dump(func_dict, f, indent=4)\n",
    "print(f\"✅ Function dictionary saved to {func_dict_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d4034bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import json\n",
    "\n",
    "original_path = os.path.join(\"outputs\", f\"{dataset}_{llm}_original.json\")\n",
    "processed_path = os.path.join(\"outputs\", f\"{dataset}_{llm}.json\")\n",
    "\n",
    "with open(original_path, 'r') as f:\n",
    "    original = json.load(f)\n",
    "\n",
    "with open(processed_path, 'r') as f:\n",
    "    processed = json.load(f)\n",
    "\n",
    "# Statistics\n",
    "total_samples = len(original)\n",
    "found_matches = 0\n",
    "text_matches = 0\n",
    "tar_eq_matches = 0\n",
    "tar_number_matches = 0\n",
    "\n",
    "# Store mismatches\n",
    "mismatches = {\n",
    "    'text_mismatches': [],\n",
    "    'tar_eq_mismatches': [],\n",
    "    'tar_number_mismatches': []\n",
    "}\n",
    "\n",
    "# Create a dictionary of processed samples with text as key for faster lookup\n",
    "processed_dict = {p['text']: p for p in processed}\n",
    "\n",
    "for i, o_sample in enumerate(original):\n",
    "    # Try to find matching text in processed samples\n",
    "    if o_sample['text'] in processed_dict:\n",
    "        found_matches += 1\n",
    "        p_sample = processed_dict[o_sample['text']]\n",
    "        \n",
    "        # Compare each field\n",
    "        if o_sample['text'] == p_sample['text']:\n",
    "            text_matches += 1\n",
    "        else:\n",
    "            mismatches['text_mismatches'].append({\n",
    "                'index': i,\n",
    "                'original': o_sample['text'],\n",
    "                'processed': p_sample['text']\n",
    "            })\n",
    "\n",
    "        c = [item.replace(\"<eoe>\", \"\") for item in o_sample['tar_eq']]\n",
    "        if c == p_sample['tar_eq']:\n",
    "            tar_eq_matches += 1\n",
    "        else:\n",
    "            mismatches['tar_eq_mismatches'].append({\n",
    "                'index': i,\n",
    "                'original': o_sample['tar_eq'],\n",
    "                'processed': p_sample['tar_eq']\n",
    "            })\n",
    "        \n",
    "        n = [item.replace(\",\", \"\") for item in p_sample['tar_number']]\n",
    "        if o_sample['tar_number'] == n:\n",
    "            tar_number_matches += 1\n",
    "        else:\n",
    "            mismatches['tar_number_mismatches'].append({\n",
    "                'index': i,\n",
    "                'original': o_sample['tar_number'],\n",
    "                'processed': p_sample['tar_number']\n",
    "            })\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total original samples: {total_samples}\")\n",
    "print(f\"Matching samples found: {found_matches} ({found_matches/total_samples:.2%})\")\n",
    "print(f\"Text matches: {text_matches} ({text_matches/total_samples:.2%})\")\n",
    "print(f\"Target equation matches: {tar_eq_matches} ({tar_eq_matches/total_samples:.2%})\")\n",
    "print(f\"Target number matches: {tar_number_matches} ({tar_number_matches/total_samples:.2%})\")\n",
    "\n",
    "# Save mismatches to file\n",
    "mismatch_path = \"outputs/mismatches.json\"\n",
    "with open(mismatch_path, 'w') as f:\n",
    "    json.dump(mismatches, f, indent=4)\n",
    "print(f\"\\nMismatches saved to {mismatch_path}\")''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "52acca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Joanne is campaigning for class president and plans to distribute some campaign materials: 20 flyers and 16 buttons. She wants each classroom to receive an identical set of campaign materials, without having any materials left over. What is the greatest number of classrooms Joanne can distribute materials to?',\n",
       " 'answer': 'The final answer to the problem is GCD(20, 16) = <<gcd(20, 16)=4>>4'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ad85cf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1240\n",
       "})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "850c84fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9d8e87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert len(original) == len(processed_samples), \"Original and processed datasets have different lengths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cdb480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''text_mismatch = 0\n",
    "for o_sample, gsm, p_sample in zip(original, processed_samples, ds['train']):\n",
    "    full_text = p_sample['question'] + \" Let's think step by step. \" + gsm['text']\n",
    "\n",
    "    try:\n",
    "        assert \"Let's think step by step.\" in o_sample['text'], \"Missing 'Let's think step by step.' in original text\"\n",
    "        #assert o_sample['text'] == full_text, \"Text mismatch\"\n",
    "    except:\n",
    "        print(f\"Original: {o_sample['text']}\")\n",
    "        print(f\"Processed: {full_text}\")\n",
    "        text_mismatch += 1\n",
    "        continue\n",
    "    #assert o_sample['start_token_idx'] == p_sample['start_token_idx'], \"Start indices mismatch\"\n",
    "    #assert o_sample['end_token_idx'] == p_sample['end_token_idx'], \"End indices mismatch\"\n",
    "    #assert o_sample['tar_eq'] == p_sample['tar_eq'], \"Target equations mismatch\"\n",
    "    #assert o_sample['tar_number'] == p_sample['tar_number'], \"Target numbers mismatch\"\n",
    "\n",
    "print(f\"Text mismatches found: {text_mismatch/len(original):.2%}\")''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmt_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
